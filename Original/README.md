## Notes

This folder is empty, as we cannot share the actual videos and audio from the debates due to copyright reasons. However, we do share the post-processed audio features in as a pickle file from Python, that once loaded takes the form of a dataframe with the relevant features per utterance (see main repository [README](https://github.com/rafamestre/Multimodal-USElecDeb60To16/blob/main/README.md) file). In this GitHub release, we cannot share that file due to its large size, but it's included in the Zenodo release of the dataset, so please check out the last version there.

The videos can be downloaded from YouTube for research purposes. Check the file [YoutubeLinks.csv](https://github.com/rafamestre/Multimodal-USElecDeb60To16/blob/main/videos/YoutubeLinks.csv) for the links. However, some of the videos were manually altered or split so that they could match the transcripts and we could alignm them properly. See the paper and the file [alignment problems.xlsx](https://github.com/rafamestre/Multimodal-USElecDeb60To16/blob/main/alignment/alignment%20problems.xlsx) for more information.

In theory, the script [split_original_audio.py](https://github.com/rafamestre/Multimodal-USElecDeb60To16/blob/main/codes/split_original_audio.py) takes the videos from the Videos folder and, using the timestamps contained in the file [allDebates_withAnnotations_all_timestamps.csv](https://github.com/rafamestre/Multimodal-USElecDeb60To16/blob/main/Multimodal%20ElecDeb60To16/allDebates_withAnnotations_all_timestamps.csv), splits the audio into a *.wav* file with each individual utterance. Upon calling the script [extract_audio_features.py](https://github.com/rafamestre/Multimodal-USElecDeb60To16/blob/main/codes/extract_audio_features.py), with the default settings, it takes the audio files from this folder, extracts the features with the Librosa library and then saves the data in a dataframe in pickle format called "df_audio_features.pkl".
